{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview and Motivation  \n",
    "<i>Provide an overview of the project goals and the motivation for it. Consider that this will be read by people who did not see your project proposal. </i>\n",
    "\n",
    "We seek to collect and examine horse racing data in order to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Work \n",
    "<i>Anything that inspired you, such as a paper, a web site, or something we discussed in class.</i>\n",
    "\n",
    "We first conducted a throough study of horse racing and betting therein as follows to understand what we were working with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Questions \n",
    "<i>What questions are you trying to answer? How did these questions evolve over the course of the project? What new questions did you consider in the course of your analysis? - Data: Source, scraping method, cleanup, storage, etc. </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Analysis \n",
    "<i> What did you learn about the data? How did you answer the questions? How can you justify your answers? </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping \n",
    "We began our adventure with an extensive amount of scraping.  We used many resources to access a large amount of data and hit many hiccups along the way.  For one, the data sources we were using were extremely varied and very disorganized.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Scraping \n",
    "We were able to access the Wunderground weather API thanks to special permission from the Weather Channel.  Using this, we obtained weather conditions for every date that we had race information available for the corresponding zip codes.  \n",
    "\n",
    "An example of how to use the API for a set date and zip code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date = '20141225'\n",
    "zip_code = '11420'\n",
    "wunderground_url = 'http://api.wunderground.com/api/4a26cfc369eb7841/history_{}/q/{}.json'.format(date, zip_code)\n",
    "examp = json.loads(requests.get(wunderground_url).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided on a list of weather conditions that would be most likely to effect the racetracks on any given day.  Given Wunderground had hundreds of weather parameters, we had to limit the scope of our scrape to avoid overfitting.  Many of the parameters given were duplicates, as each parameter was available in metric or imperial units, as well as max, min, and average. For the parameters we chose, we used metric units and average amounts if available and applicable.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_data = ['fog','hail','maxhumidity','meandewptm','meanpressurem','meantempm','meanvism',\n",
    "                'meanwdird', 'meanwindspdm', 'precipm', 'rain', 'snow', 'snowdepthm','snowfallm', 'thunder',\n",
    "                'minhumidity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to format data returned from wunderground api to have only the metrics we want\n",
    "def output_dict(in_dict):\n",
    "    temp = [(elem,in_dict[elem]) for elem in in_dict.keys() if elem in weather_data]\n",
    "    return dict(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'fog': u'1',\n",
       " u'hail': u'0',\n",
       " u'maxhumidity': u'96',\n",
       " u'meandewptm': u'3',\n",
       " u'meanpressurem': u'1008',\n",
       " u'meantempm': u'10',\n",
       " u'meanvism': u'13',\n",
       " u'meanwdird': u'261',\n",
       " u'meanwindspdm': u'27',\n",
       " u'minhumidity': u'35',\n",
       " u'precipm': u'1.52',\n",
       " u'rain': u'1',\n",
       " u'snow': u'0',\n",
       " u'snowdepthm': u'0.00',\n",
       " u'snowfallm': u'0.00',\n",
       " u'thunder': u'0'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict(examp['history']['dailysummary'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AQU</td>\n",
       "      <td>1998</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AQU</td>\n",
       "      <td>1998</td>\n",
       "      <td>10</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AQU</td>\n",
       "      <td>1998</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AQU</td>\n",
       "      <td>1998</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AQU</td>\n",
       "      <td>1998</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  track  year  month  day\n",
       "0   AQU  1998     10   28\n",
       "1   AQU  1998     10   29\n",
       "2   AQU  1998     10   30\n",
       "3   AQU  1998     10   31\n",
       "4   AQU  1998     11    1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the csv of dates we want to get weather data for from previous scraping \n",
    "datesdf = pd.read_csv(\"tempdata/usadays.csv\")\n",
    "datesdf.drop('track.1',axis=1, inplace=True)\n",
    "datesdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert date components to strings and add a 0 before single digit month/days\n",
    "datesdf[['year', 'month', 'day']] = datesdf[['year', 'month', 'day']].astype(str)\n",
    "for line in datesdf.index:\n",
    "    if len(datesdf.loc[line]['month']) == 1:\n",
    "        datesdf.loc[line]['month'] = '0' + datesdf.loc[line]['month']\n",
    "    if len(datesdf.loc[line]['day']) == 1:\n",
    "        datesdf.loc[line]['day'] = '0' + datesdf.loc[line]['day']\n",
    "\n",
    "# dictionary keyed by track to store corresponding date strings\n",
    "dates_dict = {}\n",
    "\n",
    "# stores all dates on which races occurred for a given track identifier\n",
    "for track in datesdf.track.unique():\n",
    "    datestring = []\n",
    "    for row in datesdf.index:\n",
    "        if datesdf.iloc[row]['track'] == track:\n",
    "            datestring.append(str(datesdf.iloc[row]['year']) + str(datesdf.iloc[row]['month']) + \n",
    "                              str(datesdf.iloc[row]['day']))\n",
    "    dates_dict[track] = datestring \n",
    "    \n",
    "# zipcodes for all U.S. tracks, looked up manually and arranged into alphabetical order (according to \n",
    "# track abbreviations)\n",
    "zips1 = ['11420', '60005','94403', '11003', '43123', '33056', '55379','40208', '23124', '25438', '70668', \n",
    "         '92014', '19804', '42420', '98001', '70570', '70119', '14425', '91768', '62234', '49415', '94710', \n",
    "         '33009', '60804', '46013', '90305', '46176', '40510', '42134', '20725', '71111', '75050', '90720', \n",
    "         '07073', '07757', '26047', '71901', '85023', '17028', '19020', '21215', '50009', '73111', '78154', \n",
    "         '45230', '77064', '91007', '12866', '60804', '02128', '33626', '44128', '21093', '41042']\n",
    "\n",
    "# obtained by manual lookup, arranged in alphabetical order (according to 3 letter track identifier)\n",
    "locs = datesdf.track.unique()\n",
    "\n",
    "# dictionary mapping track identifiers to zipcodes\n",
    "zips_dict1 = dict(zip(locs, zips1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'locs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-add290ee1b67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mlocs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'locs' is not defined"
     ]
    }
   ],
   "source": [
    "print locs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# queries wunderground API for every track-date combination\n",
    "# stores results in dictionary keyed by (track id, date) tuple\n",
    "%%time\n",
    "weather_dict = {}\n",
    "except_list = []\n",
    "for key in dates_dict.keys():\n",
    "    for fdate in dates_dict[key]:\n",
    "        wunderground_url = 'http://api.wunderground.com/api/4a26cfc369eb7841/history_{}/q/{}.json'.format(fdate, zips_dict1[key])\n",
    "        try:\n",
    "            temp = json.loads(requests.get(wunderground_url).text)['history']['dailysummary'][0]\n",
    "            weather_dict[(key, fdate)] = output_dict(temp)\n",
    "        except:\n",
    "            except_list.append(zip(key,fdate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## WARNING: do NOT run this line again \n",
    "# weather_df.to_csv('tempdata/weather.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to save our data into a csv by running all the scraping and cleaning commands above, which gave us the following dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track</th>\n",
       "      <th>date</th>\n",
       "      <th>fog</th>\n",
       "      <th>hail</th>\n",
       "      <th>maxhumidity</th>\n",
       "      <th>meandewptm</th>\n",
       "      <th>meanpressurem</th>\n",
       "      <th>meantempm</th>\n",
       "      <th>meanvism</th>\n",
       "      <th>meanwdird</th>\n",
       "      <th>meanwindspdm</th>\n",
       "      <th>minhumidity</th>\n",
       "      <th>precipm</th>\n",
       "      <th>rain</th>\n",
       "      <th>snow</th>\n",
       "      <th>snowdepthm</th>\n",
       "      <th>snowfallm</th>\n",
       "      <th>thunder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REM</td>\n",
       "      <td>20040925</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>12</td>\n",
       "      <td>1021</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DPK</td>\n",
       "      <td>20130612</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>17</td>\n",
       "      <td>1010</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>299</td>\n",
       "      <td>14</td>\n",
       "      <td>47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FMT</td>\n",
       "      <td>20130601</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>17</td>\n",
       "      <td>1009</td>\n",
       "      <td>22</td>\n",
       "      <td>15</td>\n",
       "      <td>182</td>\n",
       "      <td>10</td>\n",
       "      <td>61</td>\n",
       "      <td>12.95</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PAR</td>\n",
       "      <td>20050225</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>1012</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BEU</td>\n",
       "      <td>20130504</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>6</td>\n",
       "      <td>1017</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>101</td>\n",
       "      <td>16</td>\n",
       "      <td>40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AQU</td>\n",
       "      <td>20061231</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>-3</td>\n",
       "      <td>1030</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>222</td>\n",
       "      <td>15</td>\n",
       "      <td>44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BEL</td>\n",
       "      <td>20110615</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>11</td>\n",
       "      <td>1011</td>\n",
       "      <td>21</td>\n",
       "      <td>16</td>\n",
       "      <td>331</td>\n",
       "      <td>13</td>\n",
       "      <td>25</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TUR</td>\n",
       "      <td>20011214</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>8</td>\n",
       "      <td>1009</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>194</td>\n",
       "      <td>19</td>\n",
       "      <td>69</td>\n",
       "      <td>9.14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ARL</td>\n",
       "      <td>20060830</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>16</td>\n",
       "      <td>1018</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "      <td>70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ARL</td>\n",
       "      <td>20030518</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>11</td>\n",
       "      <td>1019</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>63</td>\n",
       "      <td>11</td>\n",
       "      <td>66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RET</td>\n",
       "      <td>20121207</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>1010</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>191</td>\n",
       "      <td>3</td>\n",
       "      <td>59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BEL</td>\n",
       "      <td>20020919</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>17</td>\n",
       "      <td>1019</td>\n",
       "      <td>21</td>\n",
       "      <td>16</td>\n",
       "      <td>183</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GUL</td>\n",
       "      <td>20110226</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>17</td>\n",
       "      <td>1019</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>137</td>\n",
       "      <td>9</td>\n",
       "      <td>49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LDN</td>\n",
       "      <td>20020912</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>21</td>\n",
       "      <td>1013</td>\n",
       "      <td>27</td>\n",
       "      <td>9</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PEN</td>\n",
       "      <td>20010929</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   track      date  fog  hail  maxhumidity  meandewptm  meanpressurem  meantempm  meanvism  meanwdird  meanwindspdm  minhumidity precipm  rain  snow snowdepthm snowfallm  thunder\n",
       "0    REM  20040925    0     0           94          12           1021         21        11         13             8           23    0.00     0     0        NaN      0.00        0\n",
       "1    DPK  20130612    0     0           87          17           1010         24        16        299            14           47    0.00     0     0       0.00      0.00        0\n",
       "2    FMT  20130601    0     0           94          17           1009         22        15        182            10           61   12.95     1     0        NaN      0.00        1\n",
       "3    PAR  20050225    0     0           80           6           1012         13        16         30             8           38    0.00     0     0        NaN      0.00        0\n",
       "4    BEU  20130504    0     0           66           6           1017         17        16        101            16           40    0.00     0     0        NaN      0.00        0\n",
       "5    AQU  20061231    0     0           70          -3           1030          4        16        222            15           44    0.00     0     0       0.00      0.00        0\n",
       "6    BEL  20110615    0     0           80          11           1011         21        16        331            13           25       T     1     0       0.00      0.00        0\n",
       "7    TUR  20011214    0     0          100           8           1009         10         8        194            19           69    9.14     1     0        NaN      0.00        0\n",
       "8    ARL  20060830    0     0           93          16           1018         19        14         30            13           70    0.00     1     0        NaN      0.00        0\n",
       "9    ARL  20030518    0     0           96          11           1019         14        11         63            11           66    0.00     0     0        NaN      0.00        0\n",
       "10   RET  20121207    1     0          100          16           1010         18        10        191             3           59    0.00     0     0        NaN      0.00        0\n",
       "11   BEL  20020919    0     0           84          17           1019         21        16        183             5           64    0.00     0     0        NaN      0.00        0\n",
       "12   GUL  20110226    0     0           87          17           1019         23        16        137             9           49    0.00     0     0       0.00      0.00        0\n",
       "13   LDN  20020912    0     0           96          21           1013         27         9         40             3           44    0.00     0     0        NaN      0.00        0\n",
       "14   PEN  20010929    0     0          NaN         NaN            NaN        NaN       NaN         -1           NaN          NaN    0.00     0     0        NaN      0.00        0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_df = pd.read_csv(\"tempdata/weather.csv\", index_col=0)\n",
    "weather_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSERT WEATHER CLEANING IN HERE****  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Scraping\n",
    "We obtained the full race results for the three test races of interest: The Kentucky Derby, The Preakness Downs, and the Belmont Stakes.  We obtained all this data from many Wikipedia pages, which proved very frustrating, as they were atrociously messy.  We ran into many corner cases, so that took a lot of investigation to account for.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## we create any empty dictionary which will hold all the html from the wikipedia pages \n",
    "pages = {}\n",
    "\n",
    "## we create a dictionary of track years linked to the track names for the races \n",
    "## Belmont Stakes only had data from 2006 onward, while for the other two, we got data from \n",
    "## 1998 onward as that is how far our training data goes \n",
    "years1 = [str(i) for i in range(1998,2016)]\n",
    "years2 = [str(i) for i in range(2006,2016)]\n",
    "track_year_dict = {\"_Kentucky_Derby\": years1, \"_Preakness_Stakes\": years1, \"_Belmont_Stakes\": years2}\n",
    "\n",
    "## obtaining all the html pages and putting them into our dictionary pages \n",
    "for key in track_year_dict.keys():\n",
    "    for year in track_year_dict[key]:\n",
    "        pages[year+key] = requests.get(\"https://en.wikipedia.org/wiki/{}\".format(year+key)).text\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "# function to parse scraping output\n",
    "# returns 2 data frames, one for payouts and one for results for a given race in a given year\n",
    "def parser(key, page_dict):\n",
    "    soup = BeautifulSoup(page_dict[key], \"html.parser\")\n",
    "    tables = soup.find_all(\"table\", attrs={\"class\": \"wikitable\"})\n",
    "    \n",
    "    if len(tables[0].find_all(\"tr\")) <= 5:\n",
    "        table1 = tables[0].find_all(\"tr\")\n",
    "        table2 = tables[1].find_all(\"tr\")\n",
    "    else:\n",
    "        table1 = tables[1].find_all(\"tr\")\n",
    "        table2 = tables[0].find_all(\"tr\")\n",
    "    \n",
    "    t1headers = [elem.get_text() for elem in table1[0].find_all(\"th\")]\n",
    "    t2headers = [elem.get_text() for elem in table2[0].find_all(\"th\")]\n",
    "    if (key == \"2005_Kentucky_Derby\"):\n",
    "        t2headers.append(\"Time\")\n",
    "        t2headers[t2headers.index(\"Jockey\")] = \"Horse\"\n",
    "    \n",
    "    t1 = []\n",
    "    t2 = []\n",
    "    for row1 in table1[1:]:\n",
    "        r1_data = [cell.get_text() for cell in row1.find_all(\"td\")]\n",
    "        t1.append(r1_data)\n",
    "    for row2 in table2[1:]:\n",
    "        # handles cases where cells in horse column all have header tags\n",
    "        if row2.find(\"th\"):\n",
    "            r2_data = [cell.get_text() for cell in row2.find_all(\"td\")]\n",
    "            r2_data.insert(2, row2.find(\"th\").get_text())\n",
    "            t2.append(r2_data)\n",
    "        else:\n",
    "            r2_data = [cell.get_text() for cell in row2.find_all(\"td\")]\n",
    "            t2.append(r2_data)       \n",
    "    try:\n",
    "        payout = pd.DataFrame(t1, columns=t1headers)\n",
    "        results = pd.DataFrame(t2, columns=t2headers)\n",
    "    except Exception,e:\n",
    "        # handles 2015 Kentucky Derby results table, which doesn't have a header row\n",
    "        if key == \"2015_Kentucky_Derby\":\n",
    "            t1headers = [elem.get_text() for elem in table1[0].find_all(\"td\")]\n",
    "            payout = pd.DataFrame(t1, columns=t1headers)\n",
    "            results = pd.DataFrame(t2, columns=t2headers)\n",
    "        else:\n",
    "            print str(e)\n",
    "  \n",
    "    return (payout, results)\n",
    "\n",
    "# dictionary of data frames keyed by track-year string\n",
    "# values are tuples of data frames returned by parser\n",
    "bigdict = {key:parser(key, pages) for key in pages.keys()}\n",
    "\n",
    "# constructs single payouts data frame by concatenating all payout data frames contained in bigdict\n",
    "payouts_df = pd.DataFrame(columns=[\"Post\", \"Horse\", \"Win\", \"Place\", \"Show\", \"Track\", \"Year\"])\n",
    "for track in track_year_dict.keys():\n",
    "    for year in track_year_dict[track]:\n",
    "        access = year+track\n",
    "        bigdict[access][0].columns = [\"Post\", \"Horse\", \"Win\", \"Place\", \"Show\"]\n",
    "        bigdict[access][0][\"Track\"] = track\n",
    "        bigdict[access][0][\"Year\"] = year\n",
    "        payouts_df = pd.concat([payouts_df, bigdict[access][0]], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "payouts_df = pd.read_csv(\"tempdata/payouts_df.csv\", index_col=0)\n",
    "payouts_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis \n",
    "<i>What visualizations did you use to look at your data in different ways? What are the different statistical methods you considered? Justify the decisions you made, and show any major changes to your ideas. How did you reach these conclusions? </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## this function takes a payout and returns the first odds digit, assuming a comparison to 1, i.e. x-1 \n",
    "## of that horse to win when betting occured assuming original bet of $2 and assumed take by race track of 15% \n",
    "def payoff_to_odds(payoff, bet_amount=2.0, take = .15):\n",
    "    return round(((payoff/(1-take) - bet_amount)/bet_amount),4)\n",
    "\n",
    "def odds_to_percent(odds): \n",
    "    return (float(str(odds)[2]))/(float(str(odds)[0])+float((str(odds)[2])))\n",
    "\n",
    "def normalize_odds(odds): \n",
    "    x = odds.split(\"-\")\n",
    "    if len(x) > 1: \n",
    "        return float(x[0])/float(x[1])\n",
    "    else: \n",
    "        return float(x[0])\n",
    "    \n",
    "def make_favorite(string): \n",
    "    if \"favorite\" in string: \n",
    "        return True \n",
    "    else\n",
    "        return False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
