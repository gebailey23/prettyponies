{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview and Motivation  \n",
    "<i>Provide an overview of the project goals and the motivation for it. Consider that this will be read by people who did not see your project proposal. </i>\n",
    "\n",
    "We seek to collect and examine horse racing data in order to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Work \n",
    "<i>Anything that inspired you, such as a paper, a web site, or something we discussed in class.</i>\n",
    "\n",
    "We first conducted a throough study of horse racing and betting therein as follows to understand what we were working with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Questions \n",
    "<i>What questions are you trying to answer? How did these questions evolve over the course of the project? What new questions did you consider in the course of your analysis? - Data: Source, scraping method, cleanup, storage, etc. </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Analysis \n",
    "<i> What did you learn about the data? How did you answer the questions? How can you justify your answers? </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import string\n",
    "# from tqdm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping \n",
    "We began our adventure with an extensive amount of scraping.  We used many resources to access a large amount of data and hit many hiccups along the way.  For one, the data sources we were using were extremely varied and very disorganized.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Race History Scraping \n",
    "We obtained a significant amount of information on horse races across the US since 1998 from the website racingchannel.com.  The scraping process was complicated and long, but proved very useful.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gets main page's HTML\n",
    "mainpagelink = 'http://racingchannel.com/results_archive.php'\n",
    "mainpagehtml = requests.get(mainpagelink).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started at the mainpage of racingchannel.com (see below) to get a list of racetracks and racetrackcodes, and we then created a dict of the two, which was the basis for most of our subsequent scraping. This page on the website has hyperlinks to all the individual racetrack pages, so the first step of scraping was from here.  \n",
    "<img src=\"images/image_1.png\" width=\"600px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "racetrackshtmllst = BeautifulSoup(mainpagehtml, 'html.parser').body.table.findAll('tr')[4].findAll('td')\n",
    "tracks = []\n",
    "trackcodes = []\n",
    "trackdatalinks = []\n",
    "for tracklst in racetrackshtmllst:\n",
    "    tracklinks = tracklst.findAll('a')\n",
    "    for link in tracklinks:\n",
    "        trackdatalinks.append('http://racingchannel.com/'  + link.get('href').encode('ascii'))\n",
    "        trackcodes.append(link.get('href')[10:13].encode('ascii'))\n",
    "        tracks.append(link.text.encode('ascii'))\n",
    "tracks[1] = 'Arlington Park'\n",
    "trackcodedict = dict(zip(tracks, trackcodes))\n",
    "tracklinkdict = dict(zip(tracks, trackdatalinks))\n",
    "trackinfodf = pd.DataFrame(index=tracks)\n",
    "trackinfodf['code'] = trackcodes\n",
    "trackinfodf['link'] = trackdatalinks\n",
    "trackinfodf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we then pulled the html of each individual track from the pages pulled above and beautifulSouped it.   The below code stores the cleaned html of all track info pages into a csv.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## WARNING: DO NOT RERUN THIS IF TEMPDATA ALREADY HAS TRACKINFO.CSV IN IT! ##\n",
    "\n",
    "htmloftracks = []\n",
    "for link in trackdatalinks:\n",
    "    htmloftracks.append(BeautifulSoup(requests.get(link).text, 'html.parser'))\n",
    "    print link\n",
    "    time.sleep(5)\n",
    "\n",
    "trackinfodf['track_html'] = htmloftracks\n",
    "trackinfodf.head(10)\n",
    "\n",
    "# Stores track codes and links in csv\n",
    "trackinfodf.to_csv('tempdata/trackinfo.csv', index_label='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loads trackinfodf from the csv in tempdata\n",
    "try:\n",
    "    del trackinfodf\n",
    "except:\n",
    "    pass\n",
    "trackinfodf = pd.read_csv('tempdata/trackinfo.csv', index_col='name')\n",
    "trackinfodf['track_html'] = trackinfodf['track_html'].apply(lambda h:BeautifulSoup(h, 'html.parser'))\n",
    "trackinfodf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we stored all the years and months that races occured for each track.  From the above links, we are directed to a page of months and years, that each then direct to monthly calendars with races listed on the corresponding dates. Using this page, we scraped the needed years and months. A sample page for the track Aqueduct can be seen below. \n",
    "\n",
    "<img src=\"images/image_2.png\" width=\"600px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For each track rip the years and months that the track had races ran on it\n",
    "tracksoups = trackinfodf['track_html']\n",
    "yrmonthdicts_by_track = []\n",
    "for soup in tracksoups:\n",
    "    soup = soup.body.center.findAll('table')[-1].tr.findAll('td')\n",
    "    years = []\n",
    "    yrmonths_for_track = []\n",
    "    for col in soup:\n",
    "        year = col.text.encode('ascii')[:4]\n",
    "        years.append(year)\n",
    "        months = [elem.get('href').encode('ascii')[19:21] for elem in col.findAll('a')]\n",
    "        yrmonths_for_track.append({year: months})\n",
    "\n",
    "    yrmonthdicts_by_track.append(yrmonths_for_track)\n",
    "biggie = dict(zip(trackcodes,yrmonthdicts_by_track))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above obtained years and months, we next had to pull all the days from each calendar of races.  A sample month for October 1998 for the racetrack Aqueduct can be seen below.  \n",
    "<img src=\"images/image_3.png\" width=\"600px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scrapes all the days that the track was in operation for each month, year, and track combo\n",
    "%time\n",
    "trackday_lists = []\n",
    "for track in sorted(biggie.keys()):\n",
    "    for yeardict in biggie[track]:\n",
    "        for month in yeardict.values()[0]:\n",
    "            monthhtml = BeautifulSoup(requests.get('http://racingchannel.com/archives/{}/{}/{}/default.html'\\\n",
    "                                         .format(track,yeardict.keys()[0],month)).text, 'html.parser').body.center\n",
    "            try:\n",
    "                soup = monthhtml.findAll('table')[1]\n",
    "            except:\n",
    "                soup = monthhtml.findAll('table')[0]\n",
    "            days = [daytag.get('href')[29:31].encode('ascii') for daytag in soup.findAll('a')]\n",
    "            mo = daytag.get('href')[19:21].encode('ascii')\n",
    "            yr = daytag.get('href')[14:18].encode('ascii')\n",
    "            trk = daytag.get('href')[22:25].encode('ascii')\n",
    "            for day in days:\n",
    "                trackday_lists.append([trk, yr, mo, day]) \n",
    "            time.sleep(.00001)\n",
    "\n",
    "    print track\n",
    "    \n",
    "print len(trackday_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stores a csv of the different days that races were run since 1998 at each track\n",
    "# (scraped from the racingchannel.com website in order to make accessing the html easier)\n",
    "trackdaylistdf = pd.DataFrame(trackday_lists)\n",
    "trackdaylistdf.to_csv('tempdata/alldays_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reads back in the list of lists that contains all the days we need to request from racingchannel.com\n",
    "bigdaydf = pd.read_csv('tempdata/alldays_new.csv')\n",
    "bigdaydf = bigdaydf[bigdaydf.columns[-4:]]\n",
    "bigdaydf.columns = ['track','year','month','day']\n",
    "bigdaydf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ensure we have all the tracks\n",
    "print tracks\n",
    "bigdaydf.track.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made a decision to drop the international tracks for several reasons.  Firstly, we wanted to ensure the continuity of our data and there was spotty data from the international arenas.  Secondly, the weather data was hard to obtain for international locations, so it was dangerous.  Thirdly, considering the animal travel laws, it seemed difficult for international races to have significant impacts on our races as horses must be quarantined for extended periods of time whenever they cross international borders.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop international tracks\n",
    "bigdaydf = bigdaydf.set_index(bigdaydf['track'])\n",
    "usadaydf = bigdaydf.drop(['GB1','ZA1','ZA2','AU1','AU2','FTE','HAS','NCF','WOB'])\n",
    "print usadaydf.shape\n",
    "print bigdaydf.shape\n",
    "usadaydf.to_csv('tempdata/usadays_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the csv containing USA track days\n",
    "usadaydf = pd.read_csv('tempdata/usadays_new.csv')\n",
    "usadaydf.drop('track.1',axis=1, inplace=True)\n",
    "print usadaydf.shape # this should be like 83000\n",
    "print '\\n'\n",
    "usadaydf.head() #<--this is the one we want!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the most significant scraping began. Now that we had obtained the track names and corresponding race dates, we needed to scrape all the results for every date and every track. A sample results page for the Aqueduct track on October 28, 1998 can be seen below. \n",
    "\n",
    "<img src=\"images/image_4.png\" width=\"600px\"/>\n",
    "\n",
    "This took a significant amount of time, so we included methods to track our progress along the way.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pretime = time.time()\n",
    "superout = []\n",
    "# Getting the raw HTML text from racingchannel to be later cleaned with BeautSoup\n",
    "# The live 1-9 variables and the many if else statements that follow the main code all have to do with \n",
    "# setting and hitting checkpoints along the way\n",
    "acc = 0\n",
    "livee = True\n",
    "live1 = True\n",
    "live2 = True\n",
    "live3 = True\n",
    "live4 = True\n",
    "live5 = True\n",
    "live6 = True\n",
    "live7 = True\n",
    "live8 = True\n",
    "live9 = True\n",
    "for tup in usadaydf.itertuples():\n",
    "    time.sleep(0.05)\n",
    "    magiclink = 'https://www.racingchannel.com/archives/{}/{}/{:02}/{}{}{:02}{:02}.HTM'\\\n",
    "        .format(tup[1],tup[2],tup[3],tup[1],str(tup[2])[2:],tup[3],tup[4])\n",
    "    htmlfrompage = requests.get(magiclink).text\n",
    "    superout.append([tup[1],tup[2],tup[3],tup[4],htmlfrompage])\n",
    "    acc += 1\n",
    "    if acc > 100 and livee:\n",
    "        livee = False\n",
    "        print \"Made it {}\".format(acc)\n",
    "    elif acc > 10000 and live1:\n",
    "        live1 = False\n",
    "        print \"Made it {}\".format(acc)\n",
    "    elif acc > 20000 and live2:\n",
    "        live2 = False\n",
    "        print \"Made it {}\".format(acc)\n",
    "    elif acc > 30000 and live3:\n",
    "        live3 = False\n",
    "        print \"Made it {}\".format(acc)\n",
    "    elif acc > 40000 and live4:\n",
    "        live4 = False\n",
    "        print \"Made it {}\".format(acc)\n",
    "    elif acc > 50000 and live5:\n",
    "        live5 = False\n",
    "        print \"Made it {}\".format(acc)\n",
    "    elif acc > 60000 and live6:\n",
    "        live6 = False\n",
    "        print \"Made it {}\".format(acc)\n",
    "    elif acc > 70000 and live7:\n",
    "        live7 = False\n",
    "        print \"Made it {}\".format(acc)\n",
    "    elif acc > 80000 and live8:\n",
    "        live8 = False\n",
    "        print \"Made it {}\".format(acc)\n",
    "        print \"Home stretch!!!\"  \n",
    "    \n",
    "\n",
    "superdf = pd.DataFrame(superout)\n",
    "superdf.to_csv('tempdata/superdf.csv')\n",
    "\n",
    "print time.time()-pretime, \"seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data obtained above was MASSIVE, so we had a very hard time reading the csv we created back into our iPython notebook. We constantly had memory errors when trying to read in the csv in one chunk, so used the chunksize option in pd.read_csv to overcome this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adapted from http://stackoverflow.com/questions/17557074/memory-error-when-using-pandas-read-csv\n",
    "csv_chunks = pd.read_csv('~/Desktop/superdf.csv', sep = ',', chunksize = 5000)\n",
    "superdf = pd.concat(chunk for chunk in csv_chunks)\n",
    "superdf = superdf[superdf.columns[-5:]]\n",
    "superdf.columns = ['track','year','month','day','html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## When we saved the csv of a list, it saves it as a string.  This function then converts that \n",
    "## string back into a list. This turned out to be less useful than we thought when we wrote it.  \n",
    "def stringtolist(string):\n",
    "    elems = []\n",
    "    curr = ''\n",
    "    acc = 0\n",
    "    for char in list(string):\n",
    "        if char in '[':\n",
    "            pass\n",
    "        elif (char == ',' and list(string)[acc-1] in '>') or (char == ']'):\n",
    "            elems.append(curr)\n",
    "            curr = ''\n",
    "        else:\n",
    "            curr += char\n",
    "        acc += 1\n",
    "    return elems\n",
    "a = '[<new>asdf</new>,<yeet>asdfaghfds,FF<yeet>,<FF>masdfasdf</FF>]'\n",
    "print stringtolist(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## We used a class object to make the scraping easier.  \n",
    "\n",
    "class RaceDataSet(object):\n",
    "    \n",
    "    def __init__(self, num, datedata):\n",
    "        self.num = num\n",
    "        self.datedata = datedata\n",
    "        self.first = None\n",
    "        self.second = None\n",
    "        self.third = None\n",
    "        self.firstnum = None\n",
    "        self.secondnum = None\n",
    "        self.thirdnum = None\n",
    "        self.win = None\n",
    "        self.place1 = None\n",
    "        self.place2 = None\n",
    "        self.show1 = None\n",
    "        self.show2 = None\n",
    "        self.show3 = None\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"{} : {}.....{}.{}...{}.{}...{}.{}\\n\\t{}...{}.{}.....{}.{}.{}\\n\".format(self.datedata,self.num,self.firstnum,\\\n",
    "                    self.first,self.secondnum,self.second,self.thirdnum,self.third,self.win,self.place1,\\\n",
    "                    self.place2,self.show1,self.show2,self.show3)\n",
    "    \n",
    "    def to_list(self):\n",
    "        return [self.datedata[0],self.datedata[1],self.datedata[2],self.datedata[3],self.num,self.firstnum,\\\n",
    "                    self.first,self.secondnum,self.second,self.thirdnum,self.third,self.win,self.place1,\\\n",
    "                    self.place2,self.show1,self.show2,self.show3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next created an important function that does a significant portion of the parsing of the html.  Firstly, it uses BeautifulSoup on the html, and then it goes through all of it, takes the row elements of each race, and puts the data into a dataframe.  This function is designed to take a row from superdf (the csv of the raw html), and then parse the html of only that page. Again, memory considerations forced us to approach the problem this way, as we could not hold very much of this data in memory at a time. Going row by row solved this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract(row):\n",
    "    tags = None\n",
    "    date = None\n",
    "    nobolds = []\n",
    "    if isinstance(row, pd.Series):\n",
    "        tags = BeautifulSoup(row['html'], 'html.parser').html.body.findAll('table')\n",
    "        date = [row['track'],row['year'],row['month'],row['day']]\n",
    "    elif isinstance(row, tuple):\n",
    "        if str(row[5]) == 'nan':\n",
    "            return []\n",
    "        tags = BeautifulSoup(row[5], 'html.parser').html.body.findAll('table')\n",
    "        date = [row[1],row[2],row[3],row[4]]\n",
    "    out = []\n",
    "    for table in tags:\n",
    "        bail = False\n",
    "        if table.findAll('tr') == []:\n",
    "            continue\n",
    "        elif table.tr.findAll('b') == []:\n",
    "            continue\n",
    "        elif table.tr.td.b == None:\n",
    "            nobolds.append((table.tr.td,date)) \n",
    "        elif 'race' in string.lower(table.tr.td.b.get_text().encode('ascii')):\n",
    "            trows = table.findAll('tr')\n",
    "            if len(trows) >= 4:\n",
    "                row1 = trows[0].findAll('td')\n",
    "                row2 = trows[1].findAll('td')\n",
    "                row3 = trows[2].findAll('td')\n",
    "                row4 = trows[3].findAll('td')\n",
    "                if len(row2) > 0 and len(row3) > 0 and len(row4) > 0:\n",
    "                    for elem in [row2[0].get_text().encode('ascii'),row3[0].get_text().encode('ascii'),\\\n",
    "                        row4[0].get_text().encode('ascii')]:\n",
    "                        if '$2 D/Double' in str(elem):\n",
    "                            bail = True\n",
    "                    if not bail:\n",
    "                        if not (len(row2) < 6 or len(row3) < 6 or len(row4) < 6):\n",
    "                            racenum = row1[1].get_text().encode('ascii')\n",
    "                            obj = RaceDataSet(racenum, date)\n",
    "                            obj.firstnum = row2[1].get_text().encode('ascii')\n",
    "                            obj.first = row2[2].get_text().encode('ascii')\n",
    "                            obj.secondnum = row3[1].get_text().encode('ascii')\n",
    "                            obj.second = row3[2].get_text().encode('ascii')\n",
    "                            obj.thirdnum = row4[1].get_text().encode('ascii')\n",
    "                            obj.third = row4[2].get_text().encode('ascii')\n",
    "                            obj.win = row2[3].get_text().encode('ascii')\n",
    "                            obj.place1 = row2[4].get_text().encode('ascii')\n",
    "                            obj.place2 = row3[4].get_text().encode('ascii')\n",
    "                            obj.show1 = row2[5].get_text().encode('ascii')\n",
    "                            obj.show2 = row3[5].get_text().encode('ascii')\n",
    "                            obj.show3 = row4[5].get_text().encode('ascii')\n",
    "\n",
    "                            out.append(obj.to_list())\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleandf = []\n",
    "count = 0\n",
    "for tup in tqdm(superdf.itertuples()):\n",
    "    count += 1\n",
    "    #controls the scale of the problem during testing\n",
    "    if count < 0:\n",
    "        continue\n",
    "    elif count > 1000000:\n",
    "        break\n",
    "    cleandf += extract(tup)\n",
    "cleandf = pd.DataFrame(cleandf, columns=['track','year','month','day','racenum','firstnum','first','secondnum',\\\n",
    "                                         'second','thirdnum','third','win','place1','place2','show1','show2','show3'])\n",
    "cleandf.to_csv('tempdata/cleandf.csv', columns=['track','year','month','day','racenum','firstnum','first','secondnum',\\\n",
    "                                         'second','thirdnum','third','win','place1','place2','show1','show2','show3'])\n",
    "cleandf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleandf = pd.read_csv('tempdata/cleandf.csv')\n",
    "cleandf = cleandf[cleandf.columns[-17:]]\n",
    "cleandf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Scraping \n",
    "We were able to access the Wunderground weather API thanks to special permission from the Weather Channel.  Using this, we obtained weather conditions for every date that we had race information available for the corresponding zip codes.  \n",
    "\n",
    "An example of how to use the API for a set date and zip code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date = '20141225'\n",
    "zip_code = '11420'\n",
    "wunderground_url = 'http://api.wunderground.com/api/4a26cfc369eb7841/history_{}/q/{}.json'.format(date, zip_code)\n",
    "examp = json.loads(requests.get(wunderground_url).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided on a list of weather conditions that would be most likely to effect the racetracks on any given day.  Given Wunderground had hundreds of weather parameters, we had to limit the scope of our scrape to avoid overfitting.  Many of the parameters given were duplicates, as each parameter was available in metric or imperial units, as well as max, min, and average. For the parameters we chose, we used metric units and average amounts if available and applicable.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_data = ['fog','hail','maxhumidity','meandewptm','meanpressurem','meantempm','meanvism',\n",
    "                'meanwdird', 'meanwindspdm', 'precipm', 'rain', 'snow', 'snowdepthm','snowfallm', 'thunder',\n",
    "                'minhumidity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to format data returned from wunderground api to have only the metrics we want\n",
    "def output_dict(in_dict):\n",
    "    temp = [(elem,in_dict[elem]) for elem in in_dict.keys() if elem in weather_data]\n",
    "    return dict(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'fog': u'1',\n",
       " u'hail': u'0',\n",
       " u'maxhumidity': u'96',\n",
       " u'meandewptm': u'3',\n",
       " u'meanpressurem': u'1008',\n",
       " u'meantempm': u'10',\n",
       " u'meanvism': u'13',\n",
       " u'meanwdird': u'261',\n",
       " u'meanwindspdm': u'27',\n",
       " u'minhumidity': u'35',\n",
       " u'precipm': u'1.52',\n",
       " u'rain': u'1',\n",
       " u'snow': u'0',\n",
       " u'snowdepthm': u'0.00',\n",
       " u'snowfallm': u'0.00',\n",
       " u'thunder': u'0'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict(examp['history']['dailysummary'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AQU</td>\n",
       "      <td>1998</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AQU</td>\n",
       "      <td>1998</td>\n",
       "      <td>10</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AQU</td>\n",
       "      <td>1998</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AQU</td>\n",
       "      <td>1998</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AQU</td>\n",
       "      <td>1998</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  track  year  month  day\n",
       "0   AQU  1998     10   28\n",
       "1   AQU  1998     10   29\n",
       "2   AQU  1998     10   30\n",
       "3   AQU  1998     10   31\n",
       "4   AQU  1998     11    1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the csv of dates we want to get weather data for from previous scraping \n",
    "datesdf = pd.read_csv(\"tempdata/usadays.csv\")\n",
    "datesdf.drop('track.1',axis=1, inplace=True)\n",
    "datesdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datesdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fb39ef1445c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# convert date components to strings and add a 0 before single digit month/days\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdatesdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'day'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatesdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'day'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatesdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatesdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdatesdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'0'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdatesdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datesdf' is not defined"
     ]
    }
   ],
   "source": [
    "# convert date components to strings and add a 0 before single digit month/days\n",
    "datesdf[['year', 'month', 'day']] = datesdf[['year', 'month', 'day']].astype(str)\n",
    "for line in datesdf.index:\n",
    "    if len(datesdf.loc[line]['month']) == 1:\n",
    "        datesdf.loc[line]['month'] = '0' + datesdf.loc[line]['month']\n",
    "    if len(datesdf.loc[line]['day']) == 1:\n",
    "        datesdf.loc[line]['day'] = '0' + datesdf.loc[line]['day']\n",
    "\n",
    "# dictionary keyed by track to store corresponding date strings\n",
    "dates_dict = {}\n",
    "\n",
    "# stores all dates on which races occurred for a given track identifier\n",
    "for track in datesdf.track.unique():\n",
    "    datestring = []\n",
    "    for row in datesdf.index:\n",
    "        if datesdf.iloc[row]['track'] == track:\n",
    "            datestring.append(str(datesdf.iloc[row]['year']) + str(datesdf.iloc[row]['month']) + \n",
    "                              str(datesdf.iloc[row]['day']))\n",
    "    dates_dict[track] = datestring \n",
    "    \n",
    "# zipcodes for all U.S. tracks, looked up manually and arranged into alphabetical order (according to \n",
    "# track abbreviations)\n",
    "zips1 = ['11420', '60005','94403', '11003', '43123', '33056', '55379','40208', '23124', '25438', '70668', \n",
    "         '92014', '19804', '42420', '98001', '70570', '70119', '14425', '91768', '62234', '49415', '94710', \n",
    "         '33009', '60804', '46013', '90305', '46176', '40510', '42134', '20725', '71111', '75050', '90720', \n",
    "         '07073', '07757', '26047', '71901', '85023', '17028', '19020', '21215', '50009', '73111', '78154', \n",
    "         '45230', '77064', '91007', '12866', '60804', '02128', '33626', '44128', '21093', '41042']\n",
    "\n",
    "# obtained by manual lookup, arranged in alphabetical order (according to 3 letter track identifier)\n",
    "locs = datesdf.track.unique()\n",
    "\n",
    "# dictionary mapping track identifiers to zipcodes\n",
    "zips_dict1 = dict(zip(locs, zips1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'locs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-add290ee1b67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mlocs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'locs' is not defined"
     ]
    }
   ],
   "source": [
    "print locs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# queries wunderground API for every track-date combination\n",
    "# stores results in dictionary keyed by (track id, date) tuple\n",
    "%%time\n",
    "weather_dict = {}\n",
    "except_list = []\n",
    "for key in dates_dict.keys():\n",
    "    for fdate in dates_dict[key]:\n",
    "        wunderground_url = 'http://api.wunderground.com/api/4a26cfc369eb7841/history_{}/q/{}.json'.format(fdate, zips_dict1[key])\n",
    "        try:\n",
    "            temp = json.loads(requests.get(wunderground_url).text)['history']['dailysummary'][0]\n",
    "            weather_dict[(key, fdate)] = output_dict(temp)\n",
    "        except:\n",
    "            except_list.append(zip(key,fdate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## WARNING: do NOT run this line again \n",
    "# weather_df.to_csv('tempdata/weather.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to save our data into a csv by running all the scraping and cleaning commands above, which gave us the following dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track</th>\n",
       "      <th>date</th>\n",
       "      <th>fog</th>\n",
       "      <th>hail</th>\n",
       "      <th>maxhumidity</th>\n",
       "      <th>meandewptm</th>\n",
       "      <th>meanpressurem</th>\n",
       "      <th>meantempm</th>\n",
       "      <th>meanvism</th>\n",
       "      <th>meanwdird</th>\n",
       "      <th>meanwindspdm</th>\n",
       "      <th>minhumidity</th>\n",
       "      <th>precipm</th>\n",
       "      <th>rain</th>\n",
       "      <th>snow</th>\n",
       "      <th>snowdepthm</th>\n",
       "      <th>snowfallm</th>\n",
       "      <th>thunder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REM</td>\n",
       "      <td>20040925</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>12</td>\n",
       "      <td>1021</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DPK</td>\n",
       "      <td>20130612</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>17</td>\n",
       "      <td>1010</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>299</td>\n",
       "      <td>14</td>\n",
       "      <td>47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FMT</td>\n",
       "      <td>20130601</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>17</td>\n",
       "      <td>1009</td>\n",
       "      <td>22</td>\n",
       "      <td>15</td>\n",
       "      <td>182</td>\n",
       "      <td>10</td>\n",
       "      <td>61</td>\n",
       "      <td>12.95</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PAR</td>\n",
       "      <td>20050225</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>1012</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BEU</td>\n",
       "      <td>20130504</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>6</td>\n",
       "      <td>1017</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>101</td>\n",
       "      <td>16</td>\n",
       "      <td>40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AQU</td>\n",
       "      <td>20061231</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>-3</td>\n",
       "      <td>1030</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>222</td>\n",
       "      <td>15</td>\n",
       "      <td>44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BEL</td>\n",
       "      <td>20110615</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>11</td>\n",
       "      <td>1011</td>\n",
       "      <td>21</td>\n",
       "      <td>16</td>\n",
       "      <td>331</td>\n",
       "      <td>13</td>\n",
       "      <td>25</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TUR</td>\n",
       "      <td>20011214</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>8</td>\n",
       "      <td>1009</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>194</td>\n",
       "      <td>19</td>\n",
       "      <td>69</td>\n",
       "      <td>9.14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ARL</td>\n",
       "      <td>20060830</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>16</td>\n",
       "      <td>1018</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "      <td>70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ARL</td>\n",
       "      <td>20030518</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>11</td>\n",
       "      <td>1019</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>63</td>\n",
       "      <td>11</td>\n",
       "      <td>66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RET</td>\n",
       "      <td>20121207</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>1010</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>191</td>\n",
       "      <td>3</td>\n",
       "      <td>59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BEL</td>\n",
       "      <td>20020919</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>17</td>\n",
       "      <td>1019</td>\n",
       "      <td>21</td>\n",
       "      <td>16</td>\n",
       "      <td>183</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GUL</td>\n",
       "      <td>20110226</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>17</td>\n",
       "      <td>1019</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>137</td>\n",
       "      <td>9</td>\n",
       "      <td>49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LDN</td>\n",
       "      <td>20020912</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>21</td>\n",
       "      <td>1013</td>\n",
       "      <td>27</td>\n",
       "      <td>9</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PEN</td>\n",
       "      <td>20010929</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   track      date  fog  hail  maxhumidity  meandewptm  meanpressurem  meantempm  meanvism  meanwdird  meanwindspdm  minhumidity precipm  rain  snow snowdepthm snowfallm  thunder\n",
       "0    REM  20040925    0     0           94          12           1021         21        11         13             8           23    0.00     0     0        NaN      0.00        0\n",
       "1    DPK  20130612    0     0           87          17           1010         24        16        299            14           47    0.00     0     0       0.00      0.00        0\n",
       "2    FMT  20130601    0     0           94          17           1009         22        15        182            10           61   12.95     1     0        NaN      0.00        1\n",
       "3    PAR  20050225    0     0           80           6           1012         13        16         30             8           38    0.00     0     0        NaN      0.00        0\n",
       "4    BEU  20130504    0     0           66           6           1017         17        16        101            16           40    0.00     0     0        NaN      0.00        0\n",
       "5    AQU  20061231    0     0           70          -3           1030          4        16        222            15           44    0.00     0     0       0.00      0.00        0\n",
       "6    BEL  20110615    0     0           80          11           1011         21        16        331            13           25       T     1     0       0.00      0.00        0\n",
       "7    TUR  20011214    0     0          100           8           1009         10         8        194            19           69    9.14     1     0        NaN      0.00        0\n",
       "8    ARL  20060830    0     0           93          16           1018         19        14         30            13           70    0.00     1     0        NaN      0.00        0\n",
       "9    ARL  20030518    0     0           96          11           1019         14        11         63            11           66    0.00     0     0        NaN      0.00        0\n",
       "10   RET  20121207    1     0          100          16           1010         18        10        191             3           59    0.00     0     0        NaN      0.00        0\n",
       "11   BEL  20020919    0     0           84          17           1019         21        16        183             5           64    0.00     0     0        NaN      0.00        0\n",
       "12   GUL  20110226    0     0           87          17           1019         23        16        137             9           49    0.00     0     0       0.00      0.00        0\n",
       "13   LDN  20020912    0     0           96          21           1013         27         9         40             3           44    0.00     0     0        NaN      0.00        0\n",
       "14   PEN  20010929    0     0          NaN         NaN            NaN        NaN       NaN         -1           NaN          NaN    0.00     0     0        NaN      0.00        0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_df = pd.read_csv(\"tempdata/weather.csv\", index_col=0)\n",
    "weather_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSERT WEATHER CLEANING IN HERE****  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Scraping\n",
    "We obtained the full race results for the three test races of interest: The Kentucky Derby, The Preakness Downs, and the Belmont Stakes.  We obtained all this data from many Wikipedia pages, which proved very frustrating, as they were atrociously messy.  We ran into many corner cases, so that took a lot of investigation to account for.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## we create any empty dictionary which will hold all the html from the wikipedia pages \n",
    "pages = {}\n",
    "\n",
    "## we create a dictionary of track years linked to the track names for the races \n",
    "## Belmont Stakes only had data from 2006 onward, while for the other two, we got data from \n",
    "## 1998 onward as that is how far our training data goes \n",
    "years1 = [str(i) for i in range(1998,2016)]\n",
    "years2 = [str(i) for i in range(2006,2016)]\n",
    "track_year_dict = {\"_Kentucky_Derby\": years1, \"_Preakness_Stakes\": years1, \"_Belmont_Stakes\": years2}\n",
    "\n",
    "## obtaining all the html pages and putting them into our dictionary pages \n",
    "for key in track_year_dict.keys():\n",
    "    for year in track_year_dict[key]:\n",
    "        pages[year+key] = requests.get(\"https://en.wikipedia.org/wiki/{}\".format(year+key)).text\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "# function to parse scraping output\n",
    "# returns 2 data frames, one for payouts and one for results for a given race in a given year\n",
    "def parser(key, page_dict):\n",
    "    soup = BeautifulSoup(page_dict[key], \"html.parser\")\n",
    "    tables = soup.find_all(\"table\", attrs={\"class\": \"wikitable\"})\n",
    "    \n",
    "    if len(tables[0].find_all(\"tr\")) <= 5:\n",
    "        table1 = tables[0].find_all(\"tr\")\n",
    "        table2 = tables[1].find_all(\"tr\")\n",
    "    else:\n",
    "        table1 = tables[1].find_all(\"tr\")\n",
    "        table2 = tables[0].find_all(\"tr\")\n",
    "    \n",
    "    t1headers = [elem.get_text() for elem in table1[0].find_all(\"th\")]\n",
    "    t2headers = [elem.get_text() for elem in table2[0].find_all(\"th\")]\n",
    "    if (key == \"2005_Kentucky_Derby\"):\n",
    "        t2headers.append(\"Time\")\n",
    "        t2headers[t2headers.index(\"Jockey\")] = \"Horse\"\n",
    "    \n",
    "    t1 = []\n",
    "    t2 = []\n",
    "    for row1 in table1[1:]:\n",
    "        r1_data = [cell.get_text() for cell in row1.find_all(\"td\")]\n",
    "        t1.append(r1_data)\n",
    "    for row2 in table2[1:]:\n",
    "        # handles cases where cells in horse column all have header tags\n",
    "        if row2.find(\"th\"):\n",
    "            r2_data = [cell.get_text() for cell in row2.find_all(\"td\")]\n",
    "            r2_data.insert(2, row2.find(\"th\").get_text())\n",
    "            t2.append(r2_data)\n",
    "        else:\n",
    "            r2_data = [cell.get_text() for cell in row2.find_all(\"td\")]\n",
    "            t2.append(r2_data)       \n",
    "    try:\n",
    "        payout = pd.DataFrame(t1, columns=t1headers)\n",
    "        results = pd.DataFrame(t2, columns=t2headers)\n",
    "    except Exception,e:\n",
    "        # handles 2015 Kentucky Derby results table, which doesn't have a header row\n",
    "        if key == \"2015_Kentucky_Derby\":\n",
    "            t1headers = [elem.get_text() for elem in table1[0].find_all(\"td\")]\n",
    "            payout = pd.DataFrame(t1, columns=t1headers)\n",
    "            results = pd.DataFrame(t2, columns=t2headers)\n",
    "        else:\n",
    "            print str(e)\n",
    "  \n",
    "    return (payout, results)\n",
    "\n",
    "# dictionary of data frames keyed by track-year string\n",
    "# values are tuples of data frames returned by parser\n",
    "bigdict = {key:parser(key, pages) for key in pages.keys()}\n",
    "\n",
    "# constructs single payouts data frame by concatenating all payout data frames contained in bigdict\n",
    "payouts_df = pd.DataFrame(columns=[\"Post\", \"Horse\", \"Win\", \"Place\", \"Show\", \"Track\", \"Year\"])\n",
    "for track in track_year_dict.keys():\n",
    "    for year in track_year_dict[track]:\n",
    "        access = year+track\n",
    "        bigdict[access][0].columns = [\"Post\", \"Horse\", \"Win\", \"Place\", \"Show\"]\n",
    "        bigdict[access][0][\"Track\"] = track\n",
    "        bigdict[access][0][\"Year\"] = year\n",
    "        payouts_df = pd.concat([payouts_df, bigdict[access][0]], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "payouts_df = pd.read_csv(\"tempdata/payouts_df.csv\", index_col=0)\n",
    "payouts_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis \n",
    "<i>What visualizations did you use to look at your data in different ways? What are the different statistical methods you considered? Justify the decisions you made, and show any major changes to your ideas. How did you reach these conclusions? </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## this function takes a payout and returns the first odds digit, assuming a comparison to 1, i.e. x-1 \n",
    "## of that horse to win when betting occured assuming original bet of $2 and assumed take by race track of 15% \n",
    "def payoff_to_odds(payoff, bet_amount=2.0, take = .15):\n",
    "    return round(((payoff/(1-take) - bet_amount)/bet_amount),4)\n",
    "\n",
    "def odds_to_percent(odds): \n",
    "    return (float(str(odds)[2]))/(float(str(odds)[0])+float((str(odds)[2])))\n",
    "\n",
    "def normalize_odds(odds): \n",
    "    x = odds.split(\"-\")\n",
    "    if len(x) > 1: \n",
    "        return float(x[0])/float(x[1])\n",
    "    else: \n",
    "        return float(x[0])\n",
    "    \n",
    "def make_favorite(string): \n",
    "    if \"favorite\" in string: \n",
    "        return True \n",
    "    else\n",
    "        return False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5-1']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = \"5-1\"\n",
    "x.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
